> 搜索关键词：爬虫，图片下载，下载图片

## 前提-确认图片地址在网页源代码

先确保图片链接是在网页源代码中，而不是在js中，js加载的图片爬取方法不在本文讨论之列

**第1步：**F12找到图片的地址，复制地址 /uploads/allimg/230309/003501-1678293301b5b6.jpg

![img](https://mypic2016.oss-cn-beijing.aliyuncs.com/picGo/202303111641564.png)

**第2步：**Ctrl+U, Ctrl+F, 输入刚才的地址，确认图片网址确实在网页源代码中

![img](https://mypic2016.oss-cn-beijing.aliyuncs.com/picGo/202303141326576.png)



## 信息爬取

### xpath

https://www.yuque.com/xdd1997/gbrnie/idc0pc?singleDoc# 《xpath》

```python
import requests
from fake_useragent import UserAgent
from lxml import etree

url = "https://pic.netbian.com/tupian/31257.html"
headers = {'user-agent': UserAgent().random}
html_text = requests.get(url, headers=headers).text

r = etree.HTML(html_text)
pic_list = r.xpath('//div[@class="photo-pic"]/a/img/@src')

print(pic_list)
```

/uploads/allimg/230309/003501-1678293301b5b6.jpg

### BeautifulSoup

[BeautifulSoup | bs4](https://www.yuque.com/xdd1997/gbrnie/km0888?singleDoc# 《BeautifulSoup | bs4》)

备注：`find_all` 查找元素用class名区分时不能加`class=`

```python
import requests
from fake_useragent import UserAgent
from bs4 import BeautifulSoup  # pip install beautifulsoup4

url = "https://pic.netbian.com/tupian/31257.html"
headers = {'user-agent': UserAgent().random}
html_text = requests.get(url, headers=headers).text

soup = BeautifulSoup(html_text,"html.parser")

for link in soup.find_all("div","photo-pic"):
    print(link)
    print(link.a.img["alt"])
    print(link.a.img["src"])
```

美女长发 头饰 眼镜 耳环 项链 4k动漫壁纸

/uploads/allimg/230309/003501-1678293301b5b6.jpg

### re 正则表达式

[Python正则表达式](https://www.yuque.com/xdd1997/gbrnie/oxhek3)

```python
import re
import requests
from fake_useragent import UserAgent

url = "https://pic.netbian.com/tupian/31257.html"
headers = {'user-agent': UserAgent().random}
html_text = requests.get(url, headers=headers).text

pattern = re.compile(r'class=.*photo-pic.*src=.*\"(.*)\" data-pic=')
resu_list= pattern.findall(html_text)

print(resu_list[0])
```

/uploads/allimg/230309/003501-1678293301b5b6.jpg



## 图片下载

```python
import urllib.request

for ii in pic_url_list:
	pic_path = ii.split('/')[-1]
	urllib.request.urlretrieve(ii, pic_path)
```





## 中文乱码

### 方法1:对所有内容转码

```python
html_text = requests.get(url, headers=headers).text

# 上面这一句改为以下三句：

r = requests.get(url, headers=headers)
r.encoding = 'utf-8'   # gbk
html_text = r.text
```

### 方法2: 仅对提取的内容转码

```python
titleName = titleName.encode('iso-8859-1').decode('gbk')
```